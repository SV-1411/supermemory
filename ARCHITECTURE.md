# ğŸ—ï¸ Supermemory Architecture

Deep dive into how Supermemory works under the hood.

## ğŸ“ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Application Layer                        â”‚
â”‚  (Your App: Web, Mobile, CLI, API, etc.)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Agent Orchestrator                            â”‚
â”‚  â€¢ Routes queries to memory or LLM                              â”‚
â”‚  â€¢ Manages conversation flow                                    â”‚
â”‚  â€¢ Handles memory storage decisions                             â”‚
â”‚  â€¢ Formats context for LLM                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                              â”‚
               â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Supermemory Core       â”‚    â”‚      LLM Provider              â”‚
â”‚  â€¢ Memory management     â”‚    â”‚  â€¢ OpenAI GPT                  â”‚
â”‚  â€¢ Context building      â”‚    â”‚  â€¢ Anthropic Claude            â”‚
â”‚  â€¢ Conversation storage  â”‚    â”‚  â€¢ Google Gemini               â”‚
â”‚  â€¢ Export/import         â”‚    â”‚  â€¢ Local models (Ollama)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â€¢ Hugging Face                â”‚
           â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚
    â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Embeddingâ”‚  â”‚ Vector Store â”‚
â”‚Service  â”‚  â”‚   (FAISS)    â”‚
â”‚         â”‚  â”‚              â”‚
â”‚â€¢ Xenova â”‚  â”‚â€¢ IndexFlatL2 â”‚
â”‚â€¢ Local  â”‚  â”‚â€¢ Fast search â”‚
â”‚â€¢ Free   â”‚  â”‚â€¢ Persistent  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚  Disk Storageâ”‚
             â”‚              â”‚
             â”‚â€¢ memories.jsonâ”‚
             â”‚â€¢ faiss.index â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Data Flow

### 1. Memory Storage Flow

```
User Input
    â”‚
    â–¼
[Orchestrator receives input]
    â”‚
    â–¼
[EmbeddingService.embed(text)]
    â”‚
    â”œâ”€â†’ Load model (first time only)
    â”œâ”€â†’ Tokenize text
    â”œâ”€â†’ Generate 384-dim vector
    â”‚
    â–¼
[Create Memory object]
    â”‚
    â”œâ”€â†’ id: UUID
    â”œâ”€â†’ content: original text
    â”œâ”€â†’ embedding: [0.123, -0.456, ...]
    â”œâ”€â†’ metadata: {userId, tags, etc.}
    â”œâ”€â†’ timestamp: Date.now()
    â”‚
    â–¼
[VectorStore.add(memory)]
    â”‚
    â”œâ”€â†’ Add vector to FAISS index
    â”œâ”€â†’ Store memory in array
    â”‚
    â–¼
[Optional: Save to disk]
    â”‚
    â”œâ”€â†’ memories.json (metadata)
    â”œâ”€â†’ faiss.index (vectors)
```

### 2. Memory Retrieval Flow

```
User Query
    â”‚
    â–¼
[Orchestrator receives query]
    â”‚
    â–¼
[EmbeddingService.embed(query)]
    â”‚
    â”œâ”€â†’ Generate query vector
    â”‚
    â–¼
[VectorStore.search(queryVector, topK)]
    â”‚
    â”œâ”€â†’ FAISS similarity search
    â”œâ”€â†’ Get top K nearest neighbors
    â”œâ”€â†’ Calculate similarity scores
    â”‚
    â–¼
[Apply metadata filters]
    â”‚
    â”œâ”€â†’ Filter by userId
    â”œâ”€â†’ Filter by tags
    â”œâ”€â†’ Filter by type
    â”‚
    â–¼
[Return MemoryResult[]]
    â”‚
    â”œâ”€â†’ memory: Memory object
    â”œâ”€â†’ similarity: 0.0-1.0 score
    â”‚
    â–¼
[Build LLM Context]
    â”‚
    â”œâ”€â†’ Format memories
    â”œâ”€â†’ Add system prompt
    â”œâ”€â†’ Add current query
    â”‚
    â–¼
[Send to LLM]
    â”‚
    â–¼
[LLM generates response]
    â”‚
    â–¼
[Store conversation]
    â”‚
    â–¼
Return to user
```

## ğŸ§© Component Details

### 1. EmbeddingService

**Purpose:** Convert text to numerical vectors

**Technology:** `@xenova/transformers` (Xenova/all-MiniLM-L6-v2)

**Key Features:**
- Runs locally (no API calls)
- 384-dimensional embeddings
- Mean pooling + normalization
- ~50-100ms per embedding

**Code Flow:**
```typescript
text â†’ tokenize â†’ model inference â†’ pooling â†’ normalize â†’ vector[384]
```

**Why This Model?**
- âœ… Small size (~80MB)
- âœ… Fast inference
- âœ… Good semantic understanding
- âœ… Optimized for similarity search
- âœ… Runs in Node.js

### 2. VectorStore (FAISS)

**Purpose:** Fast similarity search over vectors

**Technology:** `faiss-node` (Facebook AI Similarity Search)

**Index Type:** `IndexFlatL2` (L2 distance, exact search)

**Key Features:**
- Exact nearest neighbor search
- O(n) search complexity (acceptable for <100K vectors)
- Persistent storage
- Low memory footprint

**Search Algorithm:**
```
1. Compute L2 distance: d = ||query - memory||Â²
2. Sort by distance (ascending)
3. Return top K results
4. Convert distance to similarity: s = 1 / (1 + d)
```

**Why FAISS?**
- âœ… Industry standard (used by Meta, OpenAI)
- âœ… Extremely fast
- âœ… Free and open source
- âœ… Scales to millions of vectors
- âœ… Multiple index types available

### 3. Supermemory Core

**Purpose:** High-level memory management

**Key Methods:**

| Method | Purpose | Returns |
|--------|---------|---------|
| `remember()` | Store single memory | `memoryId` |
| `rememberBatch()` | Store multiple memories | `memoryId[]` |
| `recall()` | Search memories | `MemoryResult[]` |
| `buildLLMContext()` | Build context for LLM | `LLMContext` |
| `rememberConversation()` | Store user+AI turn | `{userMemoryId, assistantMemoryId}` |
| `save()` | Persist to disk | `void` |
| `exportMemories()` | Export all memories | `Memory[]` |

**Memory Object Structure:**
```typescript
{
  id: "uuid-v4",
  content: "The actual text",
  embedding: [0.123, -0.456, ...], // 384 numbers
  metadata: {
    userId: "user123",
    conversationId: "conv-456",
    messageType: "user" | "assistant" | "system",
    tags: ["preference", "important"],
    importance: 0.8,
    timestamp: 1234567890,
    // ... custom fields
  },
  timestamp: 1234567890
}
```

### 4. AgentOrchestrator

**Purpose:** Coordinate memory and LLM

**Key Responsibilities:**
1. Decide when to use memory
2. Retrieve relevant memories
3. Format context for LLM
4. Call LLM with augmented prompt
5. Store response in memory

**Orchestration Flow:**
```typescript
processQuery(userQuery) {
  // 1. Retrieve memories
  memories = supermemory.recall(userQuery)
  
  // 2. Build context
  context = supermemory.buildLLMContext(userQuery, memories)
  
  // 3. Format prompt
  prompt = formatContextForPrompt(context)
  
  // 4. Call LLM
  response = llmProvider.generateResponse(prompt)
  
  // 5. Store conversation
  supermemory.rememberConversation(userQuery, response)
  
  return response
}
```

### 5. LLMProvider Interface

**Purpose:** Abstract LLM implementation

**Interface:**
```typescript
interface LLMProvider {
  name: string;
  generateResponse(prompt: string): Promise<string>;
}
```

**Why This Design?**
- âœ… Swap LLMs without changing code
- âœ… Test with mock LLMs
- âœ… Support multiple LLMs simultaneously
- âœ… Easy to add new providers

## ğŸ¯ Design Decisions

### Why Local Embeddings?

**Pros:**
- âœ… No API costs
- âœ… No rate limits
- âœ… Privacy (data never leaves your server)
- âœ… Fast (no network latency)
- âœ… Works offline

**Cons:**
- âŒ Slightly lower quality than OpenAI embeddings
- âŒ Requires CPU/memory
- âŒ Model download on first use

**Decision:** For MVP, local embeddings are perfect. You can upgrade to OpenAI embeddings later if needed.

### Why FAISS Over Other Vector DBs?

**Alternatives Considered:**
- Pinecone (cloud, costs money)
- Weaviate (complex setup)
- Milvus (heavy, requires Docker)
- ChromaDB (Python-first)
- Qdrant (good, but more complex)

**Why FAISS:**
- âœ… Simplest setup
- âœ… Fastest for <100K vectors
- âœ… Free forever
- âœ… Battle-tested
- âœ… Works in Node.js

### Why TypeScript?

**Pros:**
- âœ… Type safety prevents bugs
- âœ… Better IDE support
- âœ… Self-documenting code
- âœ… Easy refactoring
- âœ… Modern ecosystem

**Cons:**
- âŒ Compilation step
- âŒ Slightly more verbose

**Decision:** TypeScript is worth it for production code.

## ğŸ“Š Performance Characteristics

### Time Complexity

| Operation | Complexity | Typical Time |
|-----------|-----------|--------------|
| Initialize | O(1) | 2-3 seconds (model load) |
| Embed text | O(n) | 50-100ms |
| Add memory | O(1) | <1ms |
| Search (FAISS) | O(n) | <10ms for 10K memories |
| Save to disk | O(n) | 100ms for 10K memories |

### Space Complexity

| Component | Size per Memory |
|-----------|----------------|
| Embedding vector | 384 Ã— 4 bytes = 1.5 KB |
| Metadata (JSON) | ~200-500 bytes |
| **Total** | **~2 KB per memory** |

**Example:** 10,000 memories â‰ˆ 20 MB

### Scalability

| Memory Count | Search Time | Storage Size |
|--------------|-------------|--------------|
| 1,000 | <1ms | 2 MB |
| 10,000 | <10ms | 20 MB |
| 100,000 | <100ms | 200 MB |
| 1,000,000 | ~1s | 2 GB |

**Note:** For >100K memories, consider upgrading to FAISS IVF index for faster search.

## ğŸ” Security Considerations

### Data Privacy

- âœ… All data stored locally
- âœ… No external API calls (for embeddings)
- âœ… Full control over data
- âœ… GDPR-friendly (easy to delete user data)

### Best Practices

1. **Encrypt sensitive data** before storing
2. **Implement user data isolation** (filter by userId)
3. **Add authentication** to API endpoints
4. **Regular backups** of memory data
5. **Audit logs** for memory access

## ğŸš€ Optimization Opportunities

### Current Implementation (MVP)

- Simple, straightforward code
- Good for <100K memories
- No complex optimizations

### Future Optimizations

1. **Better FAISS Index**
   - Switch to IVF (Inverted File Index)
   - 10-100x faster search
   - Slight accuracy trade-off

2. **Batch Processing**
   - Embed multiple texts at once
   - Reduce model overhead

3. **Memory Compression**
   - Summarize old memories
   - Reduce storage size

4. **Caching**
   - Cache frequent queries
   - LRU cache for embeddings

5. **Async Operations**
   - Background saving
   - Lazy loading

## ğŸ§ª Testing Strategy

### Unit Tests
- EmbeddingService: vector generation
- VectorStore: CRUD operations
- Supermemory: memory management
- AgentOrchestrator: flow control

### Integration Tests
- End-to-end memory storage/retrieval
- LLM integration
- Persistence (save/load)

### Performance Tests
- Search speed with various memory counts
- Memory usage monitoring
- Concurrent operations

## ğŸ“š Further Reading

- [FAISS Documentation](https://github.com/facebookresearch/faiss)
- [Sentence Transformers](https://www.sbert.net/)
- [Vector Embeddings Explained](https://www.pinecone.io/learn/vector-embeddings/)
- [Semantic Search Guide](https://www.sbert.net/examples/applications/semantic-search/README.html)

---

This architecture is designed to be **simple, fast, and free** for MVP while remaining **scalable and production-ready** for growth.
